{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c81158-f334-4bc3-ace6-a13ca673b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4380c6c-1bfa-47d2-a159-d3bf2c08199d",
   "metadata": {},
   "source": [
    "## Adapter Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9312618-cf5b-44a6-80ce-1a1172167e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parq_to_df(bucket_name, key, s3_params):\n",
    "    \n",
    "     # Create an S3 client\n",
    "    s3 = boto3.client('s3', **s3_params[\"client_kwargs\"], endpoint_url=s3_params[\"endpoint_url\"],\n",
    "                      aws_access_key_id=s3_params[\"key\"], aws_secret_access_key=s3_params[\"secret\"])\n",
    " \n",
    "    # List objects in the bucket with a specific prefix\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=key)\n",
    "\n",
    "    # Extract object keys from the response\n",
    "    object_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "    \n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data_bytes = response['Body'].read()\n",
    "\n",
    "    table = pq.read_table(BytesIO(data_bytes))\n",
    "    df = table.to_pandas()\n",
    "    return df\n",
    "\n",
    "def write_df_to_s3(df, bucket_name: str,key: str):\n",
    "    \n",
    "    sts_client = boto3.client('sts')\n",
    "    \n",
    "    assumed_role = sts_client.assume_role(\n",
    "    RoleArn='arn:aws:iam::211125758361:role/ETL_S3',\n",
    "    RoleSessionName='SESSION_NAME')\n",
    "\n",
    "    credentials = assumed_role['Credentials']\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken'],\n",
    "    region_name='ap-south-1')\n",
    "\n",
    "    response = s3.list_buckets()\n",
    "    \n",
    "    # Convert DataFrame to PyArrow Table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Write Parquet file to an in-memory buffer\n",
    "    buffer = BytesIO()\n",
    "    pq.write_table(table, buffer)\n",
    "\n",
    "    # Upload the buffer to S3\n",
    "    s3 = boto3.client('s3')\n",
    "    buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "    s3.upload_fileobj(buffer, bucket_name, key)\n",
    "\n",
    "    #Print to confirm\n",
    "    print(f\"Data written to S3 bucket: {bucket_name}/{key}\")\n",
    "\n",
    "def read_from_s3(bucket_name: str, key: str):\n",
    "    sts_client = boto3.client('sts')\n",
    "    \n",
    "    assumed_role = sts_client.assume_role(\n",
    "    RoleArn='arn:aws:iam::211125758361:role/ETL_S3',\n",
    "    RoleSessionName='SESSION_NAME')\n",
    "\n",
    "    credentials = assumed_role['Credentials']\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken'],\n",
    "    region_name='ap-south-1')\n",
    "    \n",
    "    # Download the Parquet file from S3 into an in-memory buffer\n",
    "    buffer = BytesIO()\n",
    "    s3.download_fileobj(bucket_name, key, buffer)\n",
    "\n",
    "    # Read the Parquet file from the buffer\n",
    "    buffer.seek(0)  # Reset the buffer position to the beginning\n",
    "    table = pq.read_table(buffer)\n",
    "\n",
    "    # Convert PyArrow Table to Pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    # Now, 'df' contains the data from the Parquet file\n",
    "    return df\n",
    "\n",
    "def return_objects(s3_url, s3_params, arg_date):\n",
    "    arg_date = datetime.strptime(arg_date, '%Y-%m-%d')\n",
    "    \n",
    "   # Parse the S3 URL to get bucket name and prefix\n",
    "    s3_url_parts = s3_url.split(\"/\")\n",
    "    bucket_name = s3_url_parts[2]\n",
    "    prefix = \"/\".join(s3_url_parts[3:])\n",
    "\n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=s3_params[\"key\"],\n",
    "        aws_secret_access_key=s3_params[\"secret\"],\n",
    "        endpoint_url=s3_params[\"endpoint_url\"],\n",
    "        region_name=s3_params[\"client_kwargs\"][\"region_name\"]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # List objects in the bucket\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        try:\n",
    "            object_keys = [obj['Key'] for obj in response.get('Contents', []) if datetime.strptime(obj['Key'].split('/')[3], '%Y-%m-%d') >= arg_date]\n",
    "        except ValueError as date_parse_error:\n",
    "            print(f\"Date parsing error: {date_parse_error}\")\n",
    "            print(\"Falling back to all object keys.\")\n",
    "            # Fall back to all object keys if date parsing fails\n",
    "            object_keys = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "        \n",
    "        return object_keys\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available or not valid.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39a726-8e56-4771-b9b8-9599a4f105e4",
   "metadata": {},
   "source": [
    "## Application Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b8a63d-2123-4367-8475-a8fce7c8f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(src_bucket, object_keys):\n",
    "    df = pd.concat([read_parq_to_df(src_bucket, key = obj, s3_params=s3_params) for obj in object_keys], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def transform(df, arg_date):\n",
    "    df = df[df['date'] >= arg_date]\n",
    "    df.loc[:, 'date'] = df['date'].dt.date\n",
    "    aggregations = {\n",
    "    'open': 'mean',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'mean',\n",
    "    'volume': 'sum'\n",
    "    }\n",
    "    result_df = df.groupby('date').agg(aggregations)\n",
    "    result_df.drop('volume',axis=1,inplace=True)\n",
    "    result_df['prev_close'] = result_df['close'].shift(1)\n",
    "    result_df['change_prev_closing_%'] = ((result_df['close'] - result_df['prev_close'])/result_df['prev_close'])*100\n",
    "    return result_df\n",
    "\n",
    "def load(trg_bucket, df, trg_key, trg_format):\n",
    "    key = trg_key + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + trg_format\n",
    "    write_df_to_s3(df,trg_bucket, key)\n",
    "    return True\n",
    "\n",
    "def etl_report(src_bucket, trg_bucket, object_keys, arg_date, trg_key, trg_format):\n",
    "    df = extract(src_bucket, object_keys)\n",
    "    df = transform(df, arg_date)\n",
    "    load(trg_bucket, df, trg_key, trg_format)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e5896-6bab-4a22-be62-e8ebaabd06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6965bf3a-1b19-400a-83de-d4cdf061109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function entrypoint\n",
    "\n",
    "def main():\n",
    "    # Parameters/Configurations\n",
    "    # Later read config\n",
    "    url = \"s3://desiquant/data/candles/NIFTY50/EQ.parquet.gz\"\n",
    "    s3_params = {\n",
    "    \"endpoint_url\": \"https://cbabd13f6c54798a9ec05df5b8070a6e.r2.cloudflarestorage.com\",\n",
    "    \"key\": \"5c8ea9c516abfc78987bc98c70d2868a\", \n",
    "    \"secret\": \"0cf64f9f0b64f6008cf5efe1529c6772daa7d7d0822f5db42a7c6a1e41b3cadf\", \n",
    "    \"client_kwargs\": {\n",
    "    \"region_name\": \"auto\"},\n",
    "    }\n",
    "    key = 'etl_nifty_report_' + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + '.parquet'\n",
    "    src_bucket ='desiquant'\n",
    "    trg_bucket = 'etl-nifty50'\n",
    "    src_format = '%Y%m%d'\n",
    "    arg_date = '2023-10-10'\n",
    "    trg_key = 'etl_nifty_report_'\n",
    "    trg_format = '.parquet'\n",
    "\n",
    "    # Init\n",
    "    sts_client = boto3.client('sts')\n",
    "    \n",
    "    assumed_role = sts_client.assume_role( RoleArn='arn:aws:iam::211125758361:role/ETL_S3',\n",
    "                                       RoleSessionName='SESSION_NAME')\n",
    "\n",
    "    credentials = assumed_role['Credentials']\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken'],\n",
    "    region_name='ap-south-1')\n",
    "    \n",
    "    # run application\n",
    "    object_keys = return_objects(url, s3_params, arg_date)\n",
    "    etl_report(src_bucket, trg_bucket, object_keys, arg_date, trg_key, trg_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47e2c2c2-6705-4b1b-933d-c204c8b4ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date parsing error: time data 'EQ.parquet.gz' does not match format '%Y-%m-%d'\n",
      "Falling back to all object keys.\n",
      "Data written to S3 bucket: etl-nifty50/etl_nifty_report_20240123_124136.parquet\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc94ff-2e28-4f43-aabf-0f4184343090",
   "metadata": {},
   "source": [
    "## Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d17f00e2-ad52-49b3-9898-e26adde536a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>prev_close</th>\n",
       "      <th>change_prev_closing_%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-10-10</th>\n",
       "      <td>19652.541200</td>\n",
       "      <td>19717.8</td>\n",
       "      <td>19565.45</td>\n",
       "      <td>19652.941600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-11</th>\n",
       "      <td>19809.464933</td>\n",
       "      <td>19839.2</td>\n",
       "      <td>19756.95</td>\n",
       "      <td>19809.683600</td>\n",
       "      <td>19652.941600</td>\n",
       "      <td>0.797550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-12</th>\n",
       "      <td>19806.402400</td>\n",
       "      <td>19843.3</td>\n",
       "      <td>19772.65</td>\n",
       "      <td>19806.229467</td>\n",
       "      <td>19809.683600</td>\n",
       "      <td>-0.017437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-13</th>\n",
       "      <td>19722.966400</td>\n",
       "      <td>19805.4</td>\n",
       "      <td>19635.30</td>\n",
       "      <td>19721.895733</td>\n",
       "      <td>19806.229467</td>\n",
       "      <td>-0.425794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16</th>\n",
       "      <td>19751.463467</td>\n",
       "      <td>19781.3</td>\n",
       "      <td>19691.85</td>\n",
       "      <td>19751.343333</td>\n",
       "      <td>19721.895733</td>\n",
       "      <td>0.149314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    open     high       low         close    prev_close  \\\n",
       "date                                                                      \n",
       "2023-10-10  19652.541200  19717.8  19565.45  19652.941600           NaN   \n",
       "2023-10-11  19809.464933  19839.2  19756.95  19809.683600  19652.941600   \n",
       "2023-10-12  19806.402400  19843.3  19772.65  19806.229467  19809.683600   \n",
       "2023-10-13  19722.966400  19805.4  19635.30  19721.895733  19806.229467   \n",
       "2023-10-16  19751.463467  19781.3  19691.85  19751.343333  19721.895733   \n",
       "\n",
       "            change_prev_closing_%  \n",
       "date                               \n",
       "2023-10-10                    NaN  \n",
       "2023-10-11               0.797550  \n",
       "2023-10-12              -0.017437  \n",
       "2023-10-13              -0.425794  \n",
       "2023-10-16               0.149314  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "    \n",
    "assumed_role = sts_client.assume_role( RoleArn='arn:aws:iam::211125758361:role/ETL_S3',\n",
    "                                       RoleSessionName='SESSION_NAME')\n",
    "\n",
    "credentials = assumed_role['Credentials']\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken'],\n",
    "    region_name='ap-south-1')\n",
    "\n",
    "buffer = BytesIO()\n",
    "\n",
    "s3.download_fileobj(trg_bucket, 'etl_nifty_report_20240123_124136.parquet', buffer)\n",
    "\n",
    "# Read the Parquet file from the buffer\n",
    "# Reset the buffer position to the beginning\n",
    "buffer.seek(0) \n",
    "table = pq.read_table(buffer)\n",
    "\n",
    "# Convert PyArrow Table to Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Now, 'df' contains the data from the Parquet file\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
